# app_gradio.py

import os
import time
import torch
import numpy as np
import gradio as gr
import joblib
from docx import Document
from pathlib import Path

# –ú–æ–∏ –º–æ–¥—É–ª–∏
from src.docx_parser import FieldExtractor
from src.file_preparer import FilePreparer
from src.dataset_builder import DatasetBuilder
from src.dataset_manager import DatasetManager
from src.dataset_processor import DatasetProcessor
from src.stat_model_trainer import StatisticalModelTrainer
from src.config import PROJECT_ROOT, INPUT_DIR, TEMP_DIR, DATASET_PATH, STAT_MODEL_PATH
from src.config import DATASET_DIR, MIN_DATASET_DATE, VECTORIZER, MLB, STAT_MODEL
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MultiLabelBinarizer

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
filtered_texts_list = None      # –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
filtered_topics_list = None     # –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ —Ç–µ–º–∞—Ç–∏–∫
vectorizer = None               # –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ —Ç–µ–∫—Å—Ç–∞
mlb = None                      # –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ç–æ—Ä–∞ –º–µ—Ç–æ–∫
stat_model = None               # –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç –º–æ–¥–µ–ª–∏

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –°—Ç–∞—Ç–ú–æ–¥–µ–ª–∏.
os.makedirs(STAT_MODEL_PATH, exist_ok=True)
# –µ—Å–ª–∏ –Ω–∞ –¥–∏—Å–∫–µ –µ—Å—Ç—å –æ–±—É—á–µ–Ω–Ω–∞—è –°–ú, —Ç–æ –∑–∞–≥—Ä—É–∂–∞–µ–º –µ–µ...
if os.path.exists(VECTORIZER) and os.path.exists(MLB) and os.path.exists(STAT_MODEL):
    vectorizer = joblib.load(VECTORIZER)
    mlb = joblib.load(MLB)
    stat_model = StatisticalModelTrainer()
    stat_model.load(STAT_MODEL)
    print("‚úÖ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –∏ –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —Å –¥–∏—Å–∫–∞: {STAT_MODEL_PATH}")
# ...–∏–Ω–∞—á–µ –±—É–¥–µ–º –æ–±—É—á–∞—Ç—å
else:
    print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å. –û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å.")

# === User: 1. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ ===
def update_user_file_list():
    files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(('.txt', '.docx'))]
    return gr.update(choices=files)

# –• === –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ ===
def update_file_list():
    files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(('.docx', '.txt', '.zip', '.7z', '.tar.gz', '.tgz', '.rar'))]
    return gr.update(choices=files), "\n".join(files)

# === User: 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ ===
def process_single_file(docx_filename):
    if not docx_filename.lower().endswith('.docx'):
        return "‚ùå –í—ã–±–µ—Ä–∏—Ç–µ .docx —Ñ–∞–π–ª", "", ""

    preparer = FilePreparer()
    docx_in_temp = preparer._copy_single_file_to_temp(docx_filename)
    converted_file = preparer.convert_single_docx_in_temp(docx_in_temp)

    parser = FieldExtractor(converted_file)
    text = parser._get_document_text()
    topics = parser._get_topics()

    status_file = f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω —Ñ–∞–π–ª: {converted_file}"
    status_nodel = "\n‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—é" \
        if vectorizer is not None and mlb is not None and stat_model.model is not None \
        else "‚ùå –ù–µ —Ö–≤–∞—Ç–∞–µ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ .txt
    return (
        status_file,
        status_nodel,
        text,
        ", ".join(topics) if topics else "‚ùå –¢–µ–º–∞—Ç–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
    )
# === –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ ===
def convert_selected_files(filenames):
    if not filenames:
        return "‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –≤—ã–±—Ä–∞–Ω–æ"
    
    preparer = FilePreparer()
    #docs_path = [os.path.join(INPUT_DIR, f) for f in filenames]
    converted = preparer.prepare_files()
    return f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(converted)} —Ñ–∞–π–ª–æ–≤"

def list_input_files():
    files = os.listdir(INPUT_DIR)
    return "\n".join(files)

def list_temp_files():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ .txt –∏–∑ temp/"""
    files = [f for f in os.listdir(TEMP_DIR) if f.endswith('.txt')]
    return "\n".join(files)

def get_topics_from_file(filename):
    file_path = os.path.join(INPUT_DIR, filename)
    parser = FieldExtractor(file_path=file_path)
    topics = parser._get_topics()
    return ", ".join(topics) if topics else "‚ùå –¢–µ–º–∞—Ç–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"

def auto_refresh_on_start():
    input_files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(('.docx', '.txt', '.zip', '.7z', '.tar.gz', '.tgz', '.rar'))]
    temp_files = [f for f in os.listdir(TEMP_DIR) if f.endswith('.txt')]
    dataset_files = [f for f in os.listdir(DATASET_DIR) if f.endswith('.json')]
    return (
        "\n".join(input_files),
        f"**–§–∞–π–ª—ã –≤ 'input'** (–Ω–∞–π–¥–µ–Ω–æ: {len(input_files)})",
        "\n".join(temp_files),
        f"**–§–∞–π–ª—ã –≤ 'temp'** (–Ω–∞–π–¥–µ–Ω–æ: {len(temp_files)})",
        "\n".join(temp_files),
        f"**–§–∞–π–ª—ã –≤ 'temp'** (–Ω–∞–π–¥–µ–Ω–æ: {len(temp_files)})",
        "\n".join(dataset_files),
        f"**–î–∞—Ç–∞—Å–µ—Ç –≤ .json** (–Ω–∞–π–¥–µ–Ω–æ: {len(dataset_files)})"
    )

def run_prepare_files():
    # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä FilePreparer –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
    preparer = FilePreparer()
    paths = preparer.prepare_files()
    return f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(paths)}"

def run_prepare_files_with_progress(progress=gr.Progress()):
    from tqdm import tqdm
    from time import sleep
    
    # –®–∞–≥ 1: –∫–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã
    progress(0.2, desc="üìÅ –ö–æ–ø–∏—Ä—É–µ–º .docx / .txt")
    preparer = FilePreparer()
    preparer._copy_files_to_temp()
    print("üìÅ –§–∞–π–ª—ã —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω—ã")

    # –®–∞–≥ 2: —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –∞—Ä—Ö–∏–≤—ã
    progress(0.4, desc="üì¶ –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –∞—Ä—Ö–∏–≤—ã")
    preparer._extract_archives()
    print("üì¶ –ê—Ä—Ö–∏–≤—ã —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω—ã")

    # –®–∞–≥ 3: –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º docx
    progress(0.7, desc="üìÑ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º .docx ‚Üí .txt")
    preparer._convert_all_docx_in_temp()
    print("üìÑ .DOCX —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã")

    # –®–∞–≥ 4: —É–¥–∞–ª—è–µ–º –ª–∏—à–Ω–µ–µ
    progress(0.9, desc="üßπ –£–±–∏—Ä–∞–µ–º –º—É—Å–æ—Ä –∏–∑ temp")
    preparer._remove_non_txt()
    print("üßπ –ú—É—Å–æ—Ä —É–±—Ä–∞–Ω")

    # –®–∞–≥ 5: –∑–∞–≤–µ—Ä—à–µ–Ω–æ
    txt_files = [f for f in os.listdir(TEMP_DIR) if f.endswith('.txt')]
    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –ù–∞–π–¥–µ–Ω–æ .txt: {len(txt_files)}")
    
    # –®–∞–≥ 6: –æ–±–Ω–æ–≤–ª—è–µ–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    return auto_refresh_on_start()  # –≤–µ—Ä–Ω—ë–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ input –∏ temp

def build_dataset():
    builder = DatasetBuilder()
    temp_files = [os.path.join(TEMP_DIR, f) for f in os.listdir(TEMP_DIR) if f.endswith('.txt')]
    
    print(f"üìÑ –§–∞–π–ª–æ–≤ –¥–ª—è —Å–±–æ—Ä–∫–∏: {len(temp_files)}")
    
    if not temp_files:
        return "‚ùå –ù–µ—Ç .txt —Ñ–∞–π–ª–æ–≤ –≤ temp/ ‚Üí –Ω–µ—á–µ–≥–æ —Å–æ–±–∏—Ä–∞—Ç—å"
    
    for file in temp_files:
        print(f"111. –§–∞–π–ª {file}")
        builder.add_file(file)
    
    new_dataset = builder.build()
    dataset_size = len(new_dataset)
    
    manager = DatasetManager(DATASET_PATH)
    manager.backup_dataset()
    manager.save_dataset(new_dataset)
    
    return f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–±—Ä–∞–Ω! –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {dataset_size}"

def prepare_data_for_training(MIN_DATASET_DATE):
    global filtered_texts_list, filtered_topics_list
    manager = DatasetManager(DATASET_PATH)
    dataset = manager.load_dataset()
    filtered_texts_list, filtered_topics_list = manager.prepare_data(dataset, min_date=MIN_DATASET_DATE)
        
    print(f"–î–∞—Ç–∞—Å–µ—Ç –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞: —Ç–µ–∫—Å—Ç–æ–≤: {len(filtered_texts_list)}, –º–µ—Ç–æ–∫: {len(set(topic for topics in filtered_topics_list for topic in topics))}.")
    return (
        f"–î–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤): {len(filtered_texts_list)}",
        f"–¢–µ–º–∞—Ç–∏–∫ (–ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤): {len(set(topic for topics in filtered_topics_list for topic in topics))}"
    )

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (—É–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
def train_model(progress=gr.Progress()):
    global filtered_texts_list, filtered_topics_list
    if not (filtered_texts_list and filtered_topics_list):
        return {"error": "‚ùå –ù–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –°–Ω–∞—á–∞–ª–∞ –Ω–∞–∂–º–∏—Ç–µ '–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ'"}

    global vectorizer, mlb, stat_model
    if vectorizer and mlb and stat_model:
        return {f"‚úÖ –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞ –Ω–∞ –¥–∏—Å–∫–µ –≤ –ø–∞–ø–∫–µ model. ‚ùå –î–ª—è –æ–±—É—á–µ–Ω–∏—è –∑–∞–Ω–æ–≤–æ - –æ—á–∏—Å—Ç–∏—Ç–µ –ø–∞–ø–∫—É."}

    vectorizer = TfidfVectorizer(
        max_features=10000,
        ngram_range=(1, 2),
        lowercase=True
    )
    mlb = MultiLabelBinarizer()

    # –ó–∞–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è
    start_time = time.time()

    shared_processor = DatasetProcessor(vectorizer, mlb)
    X_train_common, y_train_common, vectorizer, mlb = shared_processor.prepare_model(filtered_texts_list, filtered_topics_list)
    progress(0.1, desc="üìö –î–∞–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω—ã")
    vector_time = time.time() - start_time
    progress(0.2, desc="–†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test")
    X_train, X_test, y_train, y_test = shared_processor.split_dataset(X_train_common, y_train_common)
    # –í—Ä–µ–º—è —Ä–∞–∑–±–∏–µ–Ω–∏—è
    progress(0.3, desc="üß† –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å")
    try:
        split_time = time.time() - vector_time
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        exit(1)

    try:
        trainer = StatisticalModelTrainer()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–∏ –ö–ª–∞—Å—Å–∞ StatisticalModelTrainer: {e}")
        exit(1)

    try:
        trainer.train(X_train, y_train)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
        exit(1)
    
    progress(0.9, desc="üß† –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏")
    train_time = time.time() - split_time
    metrics = trainer.evaluate(X_test, y_test, target_names=shared_processor.mlb.classes_)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –°—Ç–∞—Ç–ú–æ–¥–µ–ª–∏ –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
    trainer.save(STAT_MODEL)

    # –í—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è
    test_time = time.time() - train_time
    metrics["–í—Ä–µ–º—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞"] = f"{split_time:.2f} —Å–µ–∫"
    metrics["–í—Ä–µ–º—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"] = f"{vector_time:.2f} —Å–µ–∫"
    metrics["–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –°—Ç–∞—Ç–ú–æ–¥–µ–ª–∏"] = f"{train_time:.2f} —Å–µ–∫"
    metrics["–í—Ä–µ–º—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –°—Ç–∞—Ç–ú–æ–¥–µ–ª–∏"] = f"{test_time:.2f} —Å–µ–∫"
    return metrics

def predict_topics(text):
    if not text or vectorizer is None or mlb is None:
        return "‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç"

    X = vectorizer.transform([text])
    
    y_proba = stat_model.model.predict_proba(X)
    #preds_binary = (y_proba[:, 0] > 0.3).astype(int)  # –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ "1"
    preds_binary = (y_proba > 0.5).astype(int)  # –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ "1"

    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç–∫–∏
    topics = mlb.inverse_transform(preds_binary)

    return ", ".join(topics[0]) if len(topics) else "‚ùå –¢–µ–º–∞—Ç–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"

def get_file_content(filename):
    file_path = os.path.join(INPUT_DIR, filename)
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()

#####################################################################################
with gr.Blocks() as demo:
    gr.Markdown("## –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –ø—Ä–æ—Å—Ç–∞–Ω–æ–≤–∫–µ —Ä—É–±—Ä–∏–∫ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞")

    #####################################################################################
    with gr.Tab("üßæ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å"):
        gr.Markdown("### üîç –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–µ–º–∞—Ç–∏–∫ –ø–æ –æ–¥–Ω–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É")
        with gr.Row():
            with gr.Column():

                # –í—ã–±–æ—Ä —Ñ–∞–π–ª–∞ –∏ –µ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                user_file_dropdown = gr.Dropdown(
                    label="–í—ã–±–µ—Ä–∏—Ç–µ .docx-—Ñ–∞–π–ª",
                    choices=[f for f in os.listdir(INPUT_DIR) if f.endswith('.docx')],
                    interactive=True
                )
                refresh_user_btn = gr.Button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫")
                output_topics = gr.Label(label="–¢–µ–º–∞—Ç–∏–∫–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
                preview_box = gr.Textbox(label="–û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç", lines=10, max_lines=20, interactive=False)
                log_file_box = gr.Textbox(label="–°—Ç–∞—Ç—É—Å —Ñ–∞–π–ª–∞", lines=1, interactive=False)
                log_model_box = gr.Textbox(label="–°—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏", lines=1, interactive=False)

            with gr.Column():
                pred_topics = gr.Label(label="–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ —Ç–µ–º–∞—Ç–∏–∫–∏")
                predict_button = gr.Button("üîÆ –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Ç–µ–º–∞—Ç–∏–∫–∏", variant="primary")

                # –ü—Ä–∏ –Ω–∞–∂–∞—Ç–∏–∏ –Ω–∞ –∫–Ω–æ–ø–∫—É ‚Üí –∑–∞–ø—É—Å–∫ predict_topics() –ø–æ —Ç–µ–∫—Å—Ç—É –∏–∑ preview_box
                predict_button.click(fn=predict_topics, inputs=preview_box, outputs=pred_topics)

        # –ü—Ä–∏–≤—è–∑–∫–∞ —Å–æ–±—ã—Ç–∏–π
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤
        refresh_user_btn.click(fn=update_user_file_list, outputs=user_file_dropdown)
        # –ü—Ä–∏ –≤—ã–±–æ—Ä–µ —Ñ–∞–π–ª–∞ ‚Üí –∑–∞–ø—É—Å–∫ process_single_file()
        user_file_dropdown.select(fn=process_single_file, inputs=user_file_dropdown, outputs=[log_file_box, log_model_box, preview_box, output_topics])
        # –ü—Ä–∏ –Ω–∞–∂–∞—Ç–∏–∏ –Ω–∞ –∫–Ω–æ–ø–∫—É "–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å"
        predict_button.click(fn=predict_topics, inputs=preview_box, outputs=pred_topics)
        
    #####################################################################################
    with gr.Tab("–ê–¥–º–∏–Ω–∫–∞"):
        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –∏ –ª–æ–≥ –¥–µ–π—Å—Ç–≤–∏–π
        progress_bar = gr.Progress()
        
        #====================================================================================
        with gr.Accordion("üóÇÔ∏è –ü–û–î–ì–û–¢–û–í–ö–ê –§–ê–ô–õ–û–í -> .txt", open=False):
            with gr.Row():
                with gr.Column():
                    input_count = gr.Markdown(f"**–§–∞–π–ª—ã –≤ 'input'** (–Ω–∞–π–¥–µ–Ω–æ: ...)")
                    input_list = gr.Textbox(label="", lines=3, max_lines=3, interactive=False)
                with gr.Column():
                    temp_count = gr.Markdown(f"**–§–∞–π–ª—ã –≤ 'temp'** (–Ω–∞–π–¥–µ–Ω–æ: ...)")
                    temp_list = gr.Textbox(label="", lines=3, max_lines=3, interactive=False)

            refresh_bttn = gr.Button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å –æ–∫–Ω–∞")
            log_output = gr.Textbox(label="–õ–æ–≥ –¥–µ–π—Å—Ç–≤–∏–π", lines=5, interactive=False)
            convert_btn = gr.Button("üîÑ –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É .docx / –∞—Ä—Ö–∏–≤–æ–≤", variant="primary")

        # –ü—Ä–∏–≤—è–∑–∫–∞ –∫ –∫–Ω–æ–ø–∫–µ
        refresh_bttn.click(
            fn=auto_refresh_on_start,
            outputs=[input_list, input_count, temp_list, temp_count]
        )
        # –ü—Ä–∏–≤—è–∑–∫–∞ —Ñ—É–Ω–∫—Ü–∏–π
        convert_btn.click(fn=run_prepare_files_with_progress, outputs=log_output)

        #====================================================================================
        # 2. –°–±–æ—Ä–∫–∞ –Ω–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        # builder = DatasetBuilder()
        with gr.Accordion("üìö –°–ë–û–†–ö–ê –î–ê–¢–ê–°–ï–¢–ê", open=False):
            with gr.Row():
                with gr.Column():
                    temp_count_for_ds = gr.Markdown(f"**–§–∞–π–ª—ã –≤ 'temp'** (–Ω–∞–π–¥–µ–Ω–æ: ...)")
                    temp_list_for_ds = gr.Textbox(label="", lines=3, max_lines=3, interactive=False)
                with gr.Column():
                    dataset_count = gr.Markdown(f"**–î–° –≤ .json** (–Ω–∞–π–¥–µ–Ω–æ: ...)")
                    dataset_list = gr.Textbox(label="", lines=3, max_lines=3, interactive=False)

            refresh_bttn = gr.Button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å –æ–∫–Ω–∞")
            build_dataset_btn = gr.Button("üìÇ –°–æ–±—Ä–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ temp/", variant="secondary")
            dataset_status = gr.Textbox(label="–°—Ç–∞—Ç—É—Å —Å–±–æ—Ä–∫–∏", lines=3)

        # –ü—Ä–∏–≤—è–∑–∫–∞ –∫ –∫–Ω–æ–ø–∫–µ
        refresh_bttn.click(
            fn=auto_refresh_on_start,
            outputs=[input_list, input_count, temp_list, temp_count, temp_list_for_ds, temp_count_for_ds, dataset_list, dataset_count]
        )
        # –ü—Ä–∏–≤—è–∑–∫–∞ —Ñ—É–Ω–∫—Ü–∏–π
        build_dataset_btn.click(fn=build_dataset, outputs=dataset_status)

        #====================================================================================
        # 3. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–º (–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ, –±—ç–∫–∞–ø, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ)
        # manager = DatasetManager(DATASET_PATH)

        #====================================================================================
        # 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –°–ú LogisticRegression()
        # processor = DatasetProcessor()
        with gr.Accordion("üìö –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –∏ –û–ë–£–ß–ï–ù–ò–ï (–º–æ–¥–µ–ª–∏ –°–ú LogisticRegression)", open=True):
            with gr.Tab("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"):
                gr.Markdown("### üîÅ –≠—Ç–∞–ø—ã –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏—è")

                # === –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
                with gr.Row():
                    with gr.Column():
                        min_date_input = gr.Textbox(label="–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ (—Ñ–∏–ª—å—Ç—Ä)", value=MIN_DATASET_DATE, lines=1)
                prepare_btn = gr.Button("üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
                with gr.Row():
                    with gr.Column():
                        doc_count_box = gr.Textbox(label="–ö–æ–ª-–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏", interactive=False)
                    with gr.Column():
                        topic_count_box = gr.Textbox(label="–ö–æ–ª-–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–º–∞—Ç–∏–∫", interactive=False)

                # === –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===
                train_btn = gr.Button("üß† –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å", variant="primary")
                metrics_box = gr.JSON(label="–ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏")

        # –ü—Ä–∏–≤—è–∑–∫–∞ –∫ –∫–Ω–æ–ø–∫–µ
        prepare_btn.click(
            fn=prepare_data_for_training,
            inputs=min_date_input,
            outputs=[doc_count_box, topic_count_box]
        )
        
        # –ü—Ä–∏–≤—è–∑–∫–∞ —Ñ—É–Ω–∫—Ü–∏–π
        train_btn.click(fn=train_model, inputs=[], outputs=metrics_box)
        #====================================================================================

    # –ê–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
    demo.load(
        fn=auto_refresh_on_start,
        outputs=[input_list, input_count, temp_list, temp_count, temp_list_for_ds, temp_count_for_ds, dataset_list, dataset_count]
    )
    #demo.load(fn=prepare_data_for_training, inputs=min_date_input, outputs=[doc_count_box, topic_count_box])
    # --- –ü—Ä–∏ –Ω–∞–∂–∞—Ç–∏–∏ –Ω–∞ –∫–Ω–æ–ø–∫—É ---
    refresh_bttn.click(
        fn=auto_refresh_on_start,
        outputs=[input_list, input_count, temp_list, temp_count, temp_list_for_ds, temp_count_for_ds, dataset_list, dataset_count]
    )
    
demo.launch()