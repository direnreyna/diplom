# app_gradio.py

import os
import time
import torch
import numpy as np
import gradio as gr
import joblib
from tqdm import tqdm
from docx import Document
from pathlib import Path

# –ú–æ–∏ –º–æ–¥—É–ª–∏
from src.docx_parser import FieldExtractor
from src.file_preparer import FilePreparer
from src.dataset_builder import DatasetBuilder
from src.dataset_manager import DatasetManager
from src.stat_model_trainer import StatisticalModelTrainer
from src.config import PROJECT_ROOT, INPUT_DIR, TEMP_DIR, DATASET_PATH, STAT_MODEL_PATH
from src.config import DATASET_DIR, MIN_DATASET_DATE, VECTORIZER, MLB, STAT_MODEL, ALL_FILTERED_TOPICS
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MultiLabelBinarizer

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
freg_allowed_topics = dict()    # —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Ç–µ–º–∞—Ç–∏–∫
X_train = None                  # –≤—ã–±–æ—Ä–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
X_val = None                    # –≤—ã–±–æ—Ä–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
X_test = None                   # –≤—ã–±–æ—Ä–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
y_train = None                  # –≤—ã–±–æ—Ä–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
y_val = None                    # –≤—ã–±–æ—Ä–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
y_test = None                   # –≤—ã–±–æ—Ä–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
filtered_topics_list = None     # –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ —Ç–µ–º–∞—Ç–∏–∫
vectorizer = None               # –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ —Ç–µ–∫—Å—Ç–∞
mlb = None                      # –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ç–æ—Ä–∞ –º–µ—Ç–æ–∫
stat_model = None               # –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç –º–æ–¥–µ–ª–∏

# 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—ã
print("CUDA available:", torch.cuda.is_available())
print("GPU device:", torch.cuda.get_device_name(0))
torch.cuda.empty_cache()

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
device = ''

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
if torch.cuda.is_available():
    device = "cuda"  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º CUDA
    print(f"–ï—â–µ —Ä–∞–∑: GPU –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.get_device_name(0)}. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {device}")
else:
    device = "cpu"  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º –¶–ü–£
    print(f"–ï—â–µ —Ä–∞–∑: GPU –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {device}")

###### # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –°—Ç–∞—Ç–ú–æ–¥–µ–ª–∏.
###### os.makedirs(STAT_MODEL_PATH, exist_ok=True)
###### # –µ—Å–ª–∏ –Ω–∞ –¥–∏—Å–∫–µ –µ—Å—Ç—å –æ–±—É—á–µ–Ω–Ω–∞—è –°–ú, —Ç–æ –∑–∞–≥—Ä—É–∂–∞–µ–º –µ–µ...
###### if os.path.exists(VECTORIZER) and os.path.exists(MLB) and os.path.exists(STAT_MODEL):
######     vectorizer = joblib.load(VECTORIZER)
######     mlb = joblib.load(MLB)
######     stat_model = StatisticalModelTrainer()
######     stat_model.load()
######     print("‚úÖ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –∏ –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —Å –¥–∏—Å–∫–∞: {STAT_MODEL_PATH}")
###### # ...–∏–Ω–∞—á–µ –±—É–¥–µ–º –æ–±—É—á–∞—Ç—å
###### else:
######     print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å. –û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å.")

# === User: 1. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ ===
def update_user_file_list():
    files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(('.txt', '.docx'))]
    return gr.update(choices=files)

# –• === –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ ===
def update_file_list():
    files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(('.docx', '.txt', '.zip', '.7z', '.tar.gz', '.tgz', '.rar'))]
    return gr.update(choices=files), "\n".join(files)

# === User: 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ ===
def process_single_file(docx_filename):
    global freg_allowed_topics

    if not docx_filename.lower().endswith('.docx'):
        return "‚ùå –í—ã–±–µ—Ä–∏—Ç–µ .docx —Ñ–∞–π–ª", "", ""

    preparer = FilePreparer()
    docx_in_temp = preparer._copy_single_file_to_temp(docx_filename)
    converted_file = preparer.convert_single_docx_in_temp(docx_in_temp)

    parser = FieldExtractor(converted_file)
    text = parser._get_document_text()
    topics = parser._get_topics()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–µ–º–∞—Ç–∏–∫, –≤–∫–ª—é—á–µ–Ω–Ω—ã—Ö –≤ ALL_FILTERED_TOPICS
    allowed_topics = set()
    try:
        with open(ALL_FILTERED_TOPICS, "r", encoding="utf-8") as f:
            for line in f:
                if line.strip() and "[" in line:
                    topic_part = line.split("[")[1].split("]")[0]
                    topic_freq = line.split("[")[1].split("‚Üí ")[1].strip()
                    allowed_topics.add(topic_part)
                    freg_allowed_topics[topic_part] = topic_freq
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {ALL_FILTERED_TOPICS}: {e}")

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–µ–º—ã –ø–æ —Å–ø–∏—Å–∫—É —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã—Ö
    print(f"–ù–∞—à–ª–∏!: {topics}")
    filtered_topics = [topic for topic in topics if topic in allowed_topics]
    print(f"–û—Å—Ç–∞–≤–∏–ª–∏: {filtered_topics}, –ø–æ—Ç–æ–º—É —á—Ç–æ –≤ {ALL_FILTERED_TOPICS} —Ä–æ–≤–Ω–æ {len(allowed_topics)} —Ç–µ–º–∞—Ç–∏–∫.")
    
    status_file = f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω —Ñ–∞–π–ª: {converted_file}"
    status_nodel = "\n‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—é" \
        if vectorizer is not None and mlb is not None and stat_model is not None \
        else "‚ùå –ù–µ —Ö–≤–∞—Ç–∞–µ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ .txt
    return (
        status_file,
        status_nodel,
        text,
        ", ".join([t + f"({freg_allowed_topics[t]})" for t in filtered_topics]) if filtered_topics else "‚ùå –¢–µ–º–∞—Ç–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
    )
# === –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ ===
def convert_selected_files(filenames):
    if not filenames:
        return "‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –≤—ã–±—Ä–∞–Ω–æ"
    
    preparer = FilePreparer()
    #docs_path = [os.path.join(INPUT_DIR, f) for f in filenames]
    converted = preparer.prepare_files()
    return f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(converted)} —Ñ–∞–π–ª–æ–≤"

def list_input_files():
    files = os.listdir(INPUT_DIR)
    return "\n".join(files)

def list_temp_files():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ .txt –∏–∑ temp/"""
    files = [f for f in os.listdir(TEMP_DIR) if f.endswith('.txt')]
    return "\n".join(files)

def get_topics_from_file(filename):
    file_path = os.path.join(INPUT_DIR, filename)
    parser = FieldExtractor(file_path=file_path)
    topics = parser._get_topics()
    return ", ".join(topics) if topics else "‚ùå –¢–µ–º–∞—Ç–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"

def auto_refresh_on_start():
    input_files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(('.docx', '.txt', '.zip', '.7z', '.tar.gz', '.tgz', '.rar'))]
    temp_files = [f for f in os.listdir(TEMP_DIR) if f.endswith('.txt')]
    dataset_files = [f for f in os.listdir(DATASET_DIR) if f.endswith('.json')]
    return (
        "\n".join(input_files),
        f"**–§–∞–π–ª—ã –≤ 'input'** (–Ω–∞–π–¥–µ–Ω–æ: {len(input_files)})",
        "\n".join(temp_files),
        f"**–§–∞–π–ª—ã –≤ 'temp'** (–Ω–∞–π–¥–µ–Ω–æ: {len(temp_files)})",
        "\n".join(temp_files),
        f"**–§–∞–π–ª—ã –≤ 'temp'** (–Ω–∞–π–¥–µ–Ω–æ: {len(temp_files)})",
        "\n".join(dataset_files),
        f"**–î–∞—Ç–∞—Å–µ—Ç –≤ .json** (–Ω–∞–π–¥–µ–Ω–æ: {len(dataset_files)})"
    )

def run_prepare_files():
    # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä FilePreparer –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
    preparer = FilePreparer()
    paths = preparer.prepare_files()
    return f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(paths)}"

def run_prepare_files_with_progress(progress=gr.Progress()):
    from time import sleep
    
    # –®–∞–≥ 1: –∫–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã
    progress(0.2, desc="üìÅ –ö–æ–ø–∏—Ä—É–µ–º .docx / .txt")
    preparer = FilePreparer()
    preparer._copy_files_to_temp()
    print("üìÅ –§–∞–π–ª—ã —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω—ã")

    # –®–∞–≥ 2: —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –∞—Ä—Ö–∏–≤—ã
    progress(0.4, desc="üì¶ –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –∞—Ä—Ö–∏–≤—ã")
    preparer._extract_archives()
    print("üì¶ –ê—Ä—Ö–∏–≤—ã —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω—ã")

    # –®–∞–≥ 3: –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º docx
    progress(0.7, desc="üìÑ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º .docx ‚Üí .txt")
    preparer._convert_all_docx_in_temp()
    print("üìÑ .DOCX —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã")

    # –®–∞–≥ 4: —É–¥–∞–ª—è–µ–º –ª–∏—à–Ω–µ–µ
    progress(0.9, desc="üßπ –£–±–∏—Ä–∞–µ–º –º—É—Å–æ—Ä –∏–∑ temp")
    preparer._remove_non_txt()
    print("üßπ –ú—É—Å–æ—Ä —É–±—Ä–∞–Ω")

    # –®–∞–≥ 5: –∑–∞–≤–µ—Ä—à–µ–Ω–æ
    txt_files = [f for f in os.listdir(TEMP_DIR) if f.endswith('.txt')]
    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –ù–∞–π–¥–µ–Ω–æ .txt: {len(txt_files)}")
    
    # –®–∞–≥ 6: –æ–±–Ω–æ–≤–ª—è–µ–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    return auto_refresh_on_start()  # –≤–µ—Ä–Ω—ë–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ input –∏ temp

def build_dataset():
    builder = DatasetBuilder()
    temp_files = [os.path.join(TEMP_DIR, f) for f in os.listdir(TEMP_DIR) if f.endswith('.txt')]
    
    print(f"üìÑ –§–∞–π–ª–æ–≤ –¥–ª—è —Å–±–æ—Ä–∫–∏: {len(temp_files)}")
    
    if not temp_files:
        return "‚ùå –ù–µ—Ç .txt —Ñ–∞–π–ª–æ–≤ –≤ temp/ ‚Üí –Ω–µ—á–µ–≥–æ —Å–æ–±–∏—Ä–∞—Ç—å"
    # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –∏—Ç–µ—Ä–∞—Ü–∏—é –≤ tqdm –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
    for file in tqdm(temp_files, desc="–°–æ–±–∏—Ä–∞–µ–º –î–° –∏–∑ .txt –≤ –µ–¥–∏–Ω—ã–π .json", unit="—Ñ–∞–π–ª"):
        builder.add_file(file)
    
    new_dataset = builder.build()
    dataset_size = len(new_dataset)
    
    manager = DatasetManager(DATASET_PATH)
    manager.backup_dataset()
    manager.save_dataset(new_dataset)
    
    return f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–±—Ä–∞–Ω! –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {dataset_size}"

def prepare_data_for_training(MIN_DATASET_DATE):
    global X_train, X_val, X_test, y_train, y_val, y_test
    manager = DatasetManager(DATASET_PATH)
    dataset = manager.load_dataset()

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (–¥–∞—Ç–∞ / —Ä–µ–¥–∫–∏–µ —Ç–µ–º–∞—Ç–∏–∫–∏) –ø–æ–¥ –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫—É—é –º–æ–¥–µ–ª—å
    filtered_texts_list, filtered_topics_list = manager.prepare_data(dataset, min_date=MIN_DATASET_DATE)
        
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test
    X_train, X_val, X_test, y_train, y_val, y_test = manager.split_dataset(filtered_texts_list, filtered_topics_list)

    #print(f"–î–∞—Ç–∞—Å–µ—Ç –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞: —Ç–µ–∫—Å—Ç–æ–≤: {len(filtered_texts_list)}, –º–µ—Ç–æ–∫: {len(set(topic for topics in filtered_topics_list for topic in topics))}.")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(dataset)}.")
    print(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –¥–∞—Ç–µ: {len(filtered_texts_list)} (train={len(X_train)}, val={len(X_val)}, test={len(X_test)})")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–º–∞—Ç–∏–∫: {len(filtered_topics_list)} (train={len(y_train)}, val={len(y_val)}, test={len(y_test)})")

    return (
        f"–î–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤): {len(filtered_texts_list)}\n(train={len(X_train)}, val={len(X_val)}, test={len(X_test)})",
        f"–¢–µ–º–∞—Ç–∏–∫ (–ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤): {len(set(topic for topics in filtered_topics_list for topic in topics))}"
    )

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (—É–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
def train_stat_model(progress=gr.Progress()):
    global X_train, X_val, X_test, y_train, y_val, y_test
    os.makedirs(STAT_MODEL_PATH, exist_ok=True)
    trainer = StatisticalModelTrainer()
    if (os.path.exists(VECTORIZER) and os.path.exists(MLB) and os.path.exists(STAT_MODEL)):
        vectorizer, mlb, model = trainer.load()
        print(f"‚úÖ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –∏ –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —Å –¥–∏—Å–∫–∞: {STAT_MODEL_PATH}")
        return {f"‚úÖ –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞ –Ω–∞ –¥–∏—Å–∫–µ –≤ –ø–∞–ø–∫–µ {STAT_MODEL_PATH}. ‚ùå –î–ª—è –æ–±—É—á–µ–Ω–∏—è –∑–∞–Ω–æ–≤–æ - –æ—á–∏—Å—Ç–∏—Ç–µ –ø–∞–ø–∫—É."}
    # –µ—Å–ª–∏ –Ω–∞ –¥–∏—Å–∫–µ –Ω–µ—Ç –æ–±—É—á–µ–Ω–Ω–æ–π –°–ú, —Ç–æ –æ–±—É—á–∞–µ–º –µ–µ...
    else:
        vectorizer, mlb, model = trainer.vectorizer, trainer.mlb, trainer.model
        print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å...")
        start_time = time.time()            # –ó–∞–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        X_train_vectorized, y_train_binarized, vectorizer, mlb = trainer.vectorize_dataset(X_train, y_train)
        progress(0.1, desc="üìö –î–∞–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω—ã")
        vector_time = time.time() - start_time

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        progress(0.3, desc="üß† –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å")
        model = trainer.train(X_train_vectorized, y_train_binarized)
        train_time = time.time() - vector_time

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å, vectorizer –∏ mlb
        trainer.save(model, vectorizer, mlb)

    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    progress(0.9, desc="üß† –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏")
    metrics = trainer.evaluate(X_test, y_test, target_names=trainer.mlb.classes_)

    # –í—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è
    test_time = time.time() - train_time
    metrics["–í—Ä–µ–º—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"] = f"{vector_time:.2f} —Å–µ–∫"
    metrics["–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –°—Ç–∞—Ç–ú–æ–¥–µ–ª–∏"] = f"{train_time:.2f} —Å–µ–∫"
    metrics["–í—Ä–µ–º—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –°—Ç–∞—Ç–ú–æ–¥–µ–ª–∏"] = f"{test_time:.2f} —Å–µ–∫"
    return metrics

def predict_topics(text):
    global freg_allowed_topics
    os.makedirs(STAT_MODEL_PATH, exist_ok=True)
    if not (os.path.exists(VECTORIZER) and os.path.exists(MLB) and os.path.exists(STAT_MODEL)):
        return (f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å. –£–ø—Å!...")
    trainer = StatisticalModelTrainer()
    stat_model,vectorizer, mlb = trainer.load()
    print(f"‚úÖ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –∏ –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —Å –¥–∏—Å–∫–∞: {STAT_MODEL_PATH}")

    X = vectorizer.transform([text])
    print("–ù–µ–Ω—É–ª–µ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:", X.nnz)
    print(f"‚ùå –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–ª–∏ –•...–¢–∏–ø={type(X)}")
    
    y_proba = stat_model.predict_proba(X)
    print(f"‚ùå –ü—Ä–µ–¥—Å–∫–∞–∑–∞–ª–∏ y_proba...y_proba")

    # --- –î–û–ë–ê–í–õ–ï–ù–ò–ï: –≤—ã–≤–æ–¥ –¢–û–ü-—Ç–µ–º –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º ---
    print("–¢–û–ü-5 —Å–∞–º—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Ç–µ–º:")
    top_indices = np.argsort(y_proba[0])[-5:][::-1]
    for idx in top_indices:
        print(f"{mlb.classes_[idx]} ‚Üí {y_proba[0][idx]:.6f}")
    # ---

    # --- –î–û–ë–ê–í–õ–ï–ù–ò–ï: –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤ ---
    thresholds = [0.15, 0.2, 0.25, 0.3, 0.5]
    print("\n–ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ—Ä–æ–≥–∏:")
    for threshold in thresholds:
        preds_binary = (y_proba > threshold).astype(int)
        topics = mlb.inverse_transform(preds_binary)
        print(f"threshold={threshold:.3f} ‚Üí {topics[0]}")
    # ---

    preds_binary = (y_proba > 0.15).astype(int)  # –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ "1"
    print(f"‚ùå –ü–µ—Ä–µ–≤–µ–ª–∏ –æ—Ç–≤–µ—Ç –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤...preds_binary")

    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç–∫–∏
    topics = mlb.inverse_transform(preds_binary)
    print(f"‚ùå –ü–µ—Ä–µ–≤–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤ —Ç–µ–º–∞—Ç–∏–∫–∏...{topics}")

    return ", ".join([t + f"({freg_allowed_topics[t]})" for t in topics[0] ]) if len(topics) else "‚ùå –¢–µ–º–∞—Ç–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"

def get_file_content(filename):
    file_path = os.path.join(INPUT_DIR, filename)
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()

#####################################################################################
with gr.Blocks() as demo:
    gr.Markdown("## –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –ø—Ä–æ—Å—Ç–∞–Ω–æ–≤–∫–µ —Ä—É–±—Ä–∏–∫ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞")

    #####################################################################################
    with gr.Tab("üßæ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å"):
        gr.Markdown("### üîç –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–µ–º–∞—Ç–∏–∫ –ø–æ –æ–¥–Ω–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É")
        with gr.Row():
            with gr.Column():

                # –í—ã–±–æ—Ä —Ñ–∞–π–ª–∞ –∏ –µ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                user_file_dropdown = gr.Dropdown(
                    label="–í—ã–±–µ—Ä–∏—Ç–µ .docx-—Ñ–∞–π–ª",
                    choices=[f for f in os.listdir(INPUT_DIR) if f.endswith('.docx')],
                    interactive=True
                )
                refresh_user_btn = gr.Button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫")
                output_topics = gr.Label(label="–¢–µ–º–∞—Ç–∏–∫–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
                preview_box = gr.Textbox(label="–û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç", lines=10, max_lines=20, interactive=False)
                log_file_box = gr.Textbox(label="–°—Ç–∞—Ç—É—Å —Ñ–∞–π–ª–∞", lines=1, interactive=False)
                log_model_box = gr.Textbox(label="–°—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏", lines=1, interactive=False)

            with gr.Column():
                pred_topics = gr.Label(label="–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ —Ç–µ–º–∞—Ç–∏–∫–∏")
                predict_button = gr.Button("üîÆ –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Ç–µ–º–∞—Ç–∏–∫–∏", variant="primary")

                # –ü—Ä–∏ –Ω–∞–∂–∞—Ç–∏–∏ –Ω–∞ –∫–Ω–æ–ø–∫—É ‚Üí –∑–∞–ø—É—Å–∫ predict_topics() –ø–æ —Ç–µ–∫—Å—Ç—É –∏–∑ preview_box
                predict_button.click(fn=predict_topics, inputs=preview_box, outputs=pred_topics)

        # –ü—Ä–∏–≤—è–∑–∫–∞ —Å–æ–±—ã—Ç–∏–π
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤
        refresh_user_btn.click(fn=update_user_file_list, outputs=user_file_dropdown)
        # –ü—Ä–∏ –≤—ã–±–æ—Ä–µ —Ñ–∞–π–ª–∞ ‚Üí –∑–∞–ø—É—Å–∫ process_single_file()
        user_file_dropdown.select(fn=process_single_file, inputs=user_file_dropdown, outputs=[log_file_box, log_model_box, preview_box, output_topics])
        # –ü—Ä–∏ –Ω–∞–∂–∞—Ç–∏–∏ –Ω–∞ –∫–Ω–æ–ø–∫—É "–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å"
        predict_button.click(fn=predict_topics, inputs=preview_box, outputs=pred_topics)
        
    #####################################################################################
    with gr.Tab("–ê–¥–º–∏–Ω–∫–∞"):
        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –∏ –ª–æ–≥ –¥–µ–π—Å—Ç–≤–∏–π
        progress_bar = gr.Progress()
        
        #====================================================================================
        with gr.Accordion("üóÇÔ∏è –ü–û–î–ì–û–¢–û–í–ö–ê –§–ê–ô–õ–û–í -> .txt", open=False):
            with gr.Row():
                with gr.Column():
                    input_count = gr.Markdown(f"**–§–∞–π–ª—ã –≤ 'input'** (–Ω–∞–π–¥–µ–Ω–æ: ...)")
                    input_list = gr.Textbox(label="", lines=3, max_lines=3, interactive=False)
                with gr.Column():
                    temp_count = gr.Markdown(f"**–§–∞–π–ª—ã –≤ 'temp'** (–Ω–∞–π–¥–µ–Ω–æ: ...)")
                    temp_list = gr.Textbox(label="", lines=3, max_lines=3, interactive=False)

            refresh_bttn = gr.Button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å –æ–∫–Ω–∞")
            log_output = gr.Textbox(label="–õ–æ–≥ –¥–µ–π—Å—Ç–≤–∏–π", lines=5, interactive=False)
            convert_btn = gr.Button("üîÑ –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É .docx / –∞—Ä—Ö–∏–≤–æ–≤", variant="primary")

        # –ü—Ä–∏–≤—è–∑–∫–∞ –∫ –∫–Ω–æ–ø–∫–µ
        refresh_bttn.click(
            fn=auto_refresh_on_start,
            outputs=[input_list, input_count, temp_list, temp_count]
        )
        # –ü—Ä–∏–≤—è–∑–∫–∞ —Ñ—É–Ω–∫—Ü–∏–π
        convert_btn.click(fn=run_prepare_files_with_progress, outputs=log_output)

        #====================================================================================
        # 2. –°–±–æ—Ä–∫–∞ –Ω–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        # builder = DatasetBuilder()
        with gr.Accordion("üìö –°–ë–û–†–ö–ê –î–ê–¢–ê–°–ï–¢–ê", open=False):
            with gr.Row():
                with gr.Column():
                    temp_count_for_ds = gr.Markdown(f"**–§–∞–π–ª—ã –≤ 'temp'** (–Ω–∞–π–¥–µ–Ω–æ: ...)")
                    temp_list_for_ds = gr.Textbox(label="", lines=3, max_lines=3, interactive=False)
                with gr.Column():
                    dataset_count = gr.Markdown(f"**–î–° –≤ .json** (–Ω–∞–π–¥–µ–Ω–æ: ...)")
                    dataset_list = gr.Textbox(label="", lines=3, max_lines=3, interactive=False)

            refresh_bttn = gr.Button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å –æ–∫–Ω–∞")
            build_dataset_btn = gr.Button("üìÇ –°–æ–±—Ä–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ temp/", variant="secondary")
            dataset_status = gr.Textbox(label="–°—Ç–∞—Ç—É—Å —Å–±–æ—Ä–∫–∏", lines=3)

        # –ü—Ä–∏–≤—è–∑–∫–∞ –∫ –∫–Ω–æ–ø–∫–µ
        refresh_bttn.click(
            fn=auto_refresh_on_start,
            outputs=[input_list, input_count, temp_list, temp_count, temp_list_for_ds, temp_count_for_ds, dataset_list, dataset_count]
        )
        # –ü—Ä–∏–≤—è–∑–∫–∞ —Ñ—É–Ω–∫—Ü–∏–π
        build_dataset_btn.click(fn=build_dataset, outputs=dataset_status)

        #====================================================================================
        # 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –°–ú LogisticRegression()
        with gr.Accordion("üìö –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø –∏ –û–ë–£–ß–ï–ù–ò–ï (–º–æ–¥–µ–ª–∏ –°–ú LogisticRegression)", open=True):
            gr.Markdown("### üîÅ –≠—Ç–∞–ø—ã –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏—è")

            # === –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
            with gr.Row():
                with gr.Column():
                    min_date_input = gr.Textbox(label="–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ (—Ñ–∏–ª—å—Ç—Ä)", value=MIN_DATASET_DATE, lines=1)
            prepare_btn = gr.Button("üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
            with gr.Row():
                with gr.Column():
                    doc_count_box = gr.Textbox(label="–ö–æ–ª-–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏", interactive=False)
                with gr.Column():
                    topic_count_box = gr.Textbox(label="–ö–æ–ª-–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–º–∞—Ç–∏–∫", interactive=False)
            with gr.Row():
                with gr.Tab("–û–±—É—á–µ–Ω–∏–µ –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏"):
                    # === –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===
                    train_btn = gr.Button("üß† –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å", variant="primary")
                    metrics_box = gr.JSON(label="–ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏")
                with gr.Tab("–û–±—É—á–µ–Ω–∏–µ –ù–° –º–æ–¥–µ–ª–∏"):
                    doc_count_box2 = gr.Textbox(label="NN", interactive=False)


        # –ü—Ä–∏–≤—è–∑–∫–∞ –∫ –∫–Ω–æ–ø–∫–µ
        prepare_btn.click(
            fn=prepare_data_for_training,
            inputs=min_date_input,
            outputs=[doc_count_box, topic_count_box]
        )
        
        # –ü—Ä–∏–≤—è–∑–∫–∞ —Ñ—É–Ω–∫—Ü–∏–π
        train_btn.click(fn=train_stat_model, inputs=[], outputs=metrics_box)
        #====================================================================================

    # –ê–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
    demo.load(
        fn=auto_refresh_on_start,
        outputs=[input_list, input_count, temp_list, temp_count, temp_list_for_ds, temp_count_for_ds, dataset_list, dataset_count]
    )
    #demo.load(fn=prepare_data_for_training, inputs=min_date_input, outputs=[doc_count_box, topic_count_box])
    # --- –ü—Ä–∏ –Ω–∞–∂–∞—Ç–∏–∏ –Ω–∞ –∫–Ω–æ–ø–∫—É ---
    refresh_bttn.click(
        fn=auto_refresh_on_start,
        outputs=[input_list, input_count, temp_list, temp_count, temp_list_for_ds, temp_count_for_ds, dataset_list, dataset_count]
    )
    
demo.launch()