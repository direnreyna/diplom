# src/dataset_processor.py

import json
import joblib
from typing import Tuple, Any, List
from sklearn.model_selection import train_test_split
from .utils import first_date_is_newer
from .config import VECTORIZER, MLB, TOPIC_FREQ_LIMIT

class DatasetProcessor:
    def __init__(self, vectorizer, mlb):
        self.vectorizer = vectorizer
        self.mlb = mlb
        topics_list = []

    def prepare_model(self, filtered_texts_list, filtered_topics_list, mode: str = 'stat') -> Tuple:
        """
        –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞

        :param raw_data: –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–≤–∞—Ä–∏ X –∏ y
        :param mode: 'stat' ‚Äî –¥–ª—è —Å—Ç–∞—Ç–º–æ–¥–µ–ª–∏, 'bert' ‚Äî –¥–ª—è RuBERT
        :return: (X_tfidf, y_binary), —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤, —Å–ø–∏—Å–æ–∫ –º–µ—Ç–æ–∫
        """
        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
        X = self.vectorizer.fit_transform(filtered_texts_list)
        # –ë–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–º–∞—Ç–∏–∫ (–º—É–ª—å—Ç–∏–ª–µ–π–±–ª –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ)
        y = self.mlb.fit_transform(filtered_topics_list)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º vectorizer –∏ mlb
        joblib.dump(self.vectorizer, VECTORIZER)
        joblib.dump(self.mlb, MLB)
        print("üíæ vectorizer –∏ mlb —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫—É 'model'")

        return X, y, self.vectorizer, self.mlb

    def save_artifacts(self, artifact, path: str) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –Ω–∞ –¥–∏—Å–∫"""
        joblib.dump(artifact, path)
        print(f"üíæ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {path}")

    def get_label_names(self) -> list:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ç–µ–º–∞—Ç–∏–∫ (–º–µ—Ç–æ–∫)"""
        if not hasattr(self.mlb, 'classes_'):
            raise ValueError("–ú–æ–¥–µ–ª—å –µ—â—ë –Ω–µ –æ–±—É—á–µ–Ω–∞/–ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ prepare_data() —Å–Ω–∞—á–∞–ª–∞.")
        return self.mlb.classes_.tolist()

    def vectorize_text(self, text: str):
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç –≤ TF-IDF –≤–µ–∫—Ç–æ—Ä"""
        return self.vectorizer.transform([text])

    def binarize_topics(self, topics_list: list):
        """–ë–∏–Ω–∞—Ä–∏–∑—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–º–∞—Ç–∏–∫ (–¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞)"""
        return self.mlb.transform(topics_list)
    
    def split_dataset(self, X: Any, y: Any, test_size:float=0.2, random_state:int=42) -> Tuple:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ train –∏ test.
        """
        return train_test_split(X, y, test_size=test_size, random_state=random_state)
    
    def evaluate(self, X_test, y_test):
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
        """
        if not hasattr(self, 'model'):
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")

        y_pred = self.model.predict(X_test)

        metrics = {
            "precision": precision_score(y_test, y_pred, average="macro", zero_division=0),
            "recall": recall_score(y_test, y_pred, average="macro", zero_division=0),
            "f1": f1_score(y_test, y_pred, average="macro", zero_division=0),
            "hamming_loss": hamming_loss(y_test, y_pred)
        }

        print(f"Precision (macro): {metrics['precision']:.4f}")
        print(f"Recall (macro): {metrics['recall']:.4f}")
        print(f"F1-score (macro): {metrics['f1']:.4f}")
        print(f"Hamming loss: {metrics['hamming_loss']:.4f}")

        return metrics