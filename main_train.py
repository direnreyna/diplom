# main_train.py

import os
import json
import joblib
import shutil
import torch
import torch.nn as nn
import numpy as np

from tqdm import tqdm
from src.config import MAX_WORDS, MAX_LEN
from src.config import PROJECT_ROOT, INPUT_DIR, TEMP_DIR, MIN_DATASET_DATE
from src.config import DATASET_PATH, STAT_MODEL_PATH, STAT_MODEL, VECTORIZER, MLB
from src.config import NN_DATASET_PATH, NN_MODEL_PATH, NN_MODEL, NN_TOKENIZER, NN_MLB

from src.docx_parser import FieldExtractor
from src.file_preparer import FilePreparer
from src.dataset_builder import DatasetBuilder
from src.dataset_manager import DatasetManager
from src.stat_model_trainer import StatisticalModelTrainer

from datasets import Dataset
from typing import List, Dict
from scipy.sparse import csr_matrix
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from transformers import TrainingArguments, Trainer
from torch.utils.data import TensorDataset, DataLoader

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
train_model = True              # –§–ª–∞–≥ - –±—ã—Å—Ç—Ä—ã–π –ø—Ä–µ—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –Ω–∞ –¥–∏—Å–∫–µ)
choosen_model = 'nn'            # stat / nn - –≤—ã–±–æ—Ä –æ–±—É—á–∞–µ–º–æ–π/–ø—Ä–∏–º–µ–Ω—è–µ–º–æ–π –º–æ–¥–µ–ª–∏.
need_train = False              # –§–ª–∞–≥ - –Ω—É–∂–Ω–æ –ª–∏ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ –Ω–µ—Ç –Ω–∞ –¥–∏—Å–∫–µ –æ–±—É—á–µ–Ω–Ω–æ–π)
X_train_common = None           # –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
y_train_common = None           # –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç–æ–∫
vectorizer = None               # –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ —Ç–µ–∫—Å—Ç–∞
mlb = None                      # –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ç–æ—Ä–∞ –º–µ—Ç–æ–∫
stat_model = None               # –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç –º–æ–¥–µ–ª–∏
exam_is_done = None             # –≠–∫–∑–∞–º–µ–Ω –≤ –£–ò–ò —Å–¥–∞–Ω? –ü—Ä–æ–ø—É—Å–∫ –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –¥–æ—Ä–∞–±–æ—Ç–æ–∫

if __name__ == "__main__":
    if  train_model:
        #####################################################################
        # –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–ê
        #####################################################################
        #####################################################################
        # I. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –§–∞–π–ª–æ–≤
        #####################################################################
        # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ (–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ –∞—Ä—Ö–∏–≤–æ–≤ –∏/–∏–ª–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏–∑ .docx)
        
        preparer = FilePreparer()
        docs_path = preparer.prepare_files()   # —Ç—É—Ç –Ω–æ–≤—ã—Ö —É–∂–µ –Ω–µ—Ç

        #####################################################################
        # II. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        #####################################################################
        # 2. –°–±–æ—Ä–∫–∞ –Ω–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        builder = DatasetBuilder()
        
        print('–ù–∞–π–¥–µ–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –î–°: ', len(docs_path))
        for file_path in tqdm(docs_path, desc="–°–æ–±–∏—Ä–∞–µ–º –î–° –∏–∑ .txt –≤ –µ–¥–∏–Ω—ã–π .json", unit="—Ñ–∞–π–ª"):
            builder.add_file(file_path)
        new_dataset = builder.build()
        
        # 3. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–º –ø—Ä–∏ —á–∞—Å—Ç–∏—á–Ω–æ–º –ø–æ–ø–æ–ª–Ω–µ–Ω–∏–∏ (–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ, –±—ç–∫–∞–ø, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ)
        # –ë—É–¥–µ—Ç —Ä–∞–ª–∏–∑–æ–≤–∞–Ω–æ –ø–æ—Å–ª–µ —Å–¥–∞—á–∏ –≠–∫–∑–∞–º–µ–Ω–∞: –≤–∞–∂–Ω–∞—è –≤–µ—â—å, –Ω–æ –Ω–µ –Ω–∞ –Ω–∞—á–∞–ª—å–Ω–æ–º —ç—Ç–∞–ø–µ.
        updated_dataset = new_dataset
        manager = DatasetManager(DATASET_PATH)
        manager.backup_dataset()
    if 1:    
        #####################################################################
        # –û–±—Ö–æ–¥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –µ–ª—Å–∏ –µ—Å—Ç—å –¥–∞—Ç—Å–µ—Ç –≤ .json
        #####################################################################
        #### manager = DatasetManager(NN_DATASET_PATH)
        #### if (os.path.exists(NN_DATASET_PATH)):
        ####     updated_dataset = manager.load_dataset()
        #### print(f"‚úÖ–ó–∞–≥—Ä—É–∑–∏–ª –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ .json")

        #####################################################################
        # III. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        #####################################################################
        if exam_is_done:
            old_dataset = manager.load_dataset()
            if builder.is_empty():
                updated_dataset = old_dataset
                print("–ù–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ—Ç ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –¥–∞—Ç–∞—Å–µ—Ç")
            # –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–æ–≤—ã–µ, —Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ä—ã–µ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö.
            # –ò–∑–º–µ–Ω–µ–Ω–Ω—ã–µ —É–¥–∞–ª–∏—Ç—å –∏–∑ –Ω–æ–≤—ã—Ö –∏ –æ–±–Ω–æ–≤–∏—Ç—å –∏—Ö –∫–æ–ø–∏–∏ –≤ —Å—Ç–∞—Ä–æ–º –î–°
            else:
                updated_dataset, updated_new_items = manager.update_dataset(old_dataset, new_dataset)
                print(f"–î–æ–±–∞–≤–ª–µ–Ω–æ –∏–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–æ {len(updated_dataset) - len(old_dataset)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            manager.save_dataset(updated_dataset)
        else:
            manager.save_dataset(updated_dataset)
        print(f"‚úÖ–°–æ—Ö—Ä–∞–Ω–∏–ª –¥–∞—Ç–∞—Å–µ—Ç –≤ .json")
            
        # 4. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (–¥–∞—Ç–∞ / —Ä–µ–¥–∫–∏–µ —Ç–µ–º–∞—Ç–∏–∫–∏) –ø–æ–¥ –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫—É—é –º–æ–¥–µ–ª—å
        filtered_texts_list, filtered_topics_list = manager.prepare_data(updated_dataset, min_date=MIN_DATASET_DATE)
        print(f"‚úÖ–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–ª –î–°")
        print(f"{len(filtered_texts_list)}, {len(filtered_topics_list)}")

        # 5. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test
        X_train, X_val, X_test, y_train, y_val, y_test = manager.split_dataset(filtered_texts_list, filtered_topics_list)
        print(f"‚úÖ–†–∞–∑–¥–µ–ª–∏–ª –Ω–∞ train/val/test")
        
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(updated_dataset)}.")
        print(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –¥–∞—Ç–µ: {len(filtered_texts_list)} (train={len(X_train)}, val={len(X_val)}, test={len(X_test)})")

        #####################################################################
        # –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ê–Ø –ú–û–î–ï–õ–¨ LogisticRegression()
        #####################################################################
        if choosen_model == 'stat':
            #####################################################################
            # IV. –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
            #####################################################################
            # 5. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –¥–∏—Å–∫–∞...
            os.makedirs(STAT_MODEL_PATH, exist_ok=True)
            trainer = StatisticalModelTrainer()
            if (os.path.exists(VECTORIZER) and os.path.exists(MLB) and os.path.exists(STAT_MODEL)):
                vectorizer, mlb, model = trainer.load()
                print(f"‚úÖ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –∏ –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —Å –¥–∏—Å–∫–∞: {STAT_MODEL_PATH}")
            # –µ—Å–ª–∏ –Ω–∞ –¥–∏—Å–∫–µ –Ω–µ—Ç –æ–±—É—á–µ–Ω–Ω–æ–π –°–ú, —Ç–æ –æ–±—É—á–∞–µ–º –µ–µ...
            else:
                vectorizer, mlb, model = trainer.vectorizer, trainer.mlb, trainer.model
                print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å...")

                # 6. –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
                X_train_vectorized, y_train_binarized, vectorizer, mlb = trainer.vectorize_dataset(X_train, y_train)

                # 8. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
                model = trainer.train(X_train_vectorized, y_train_binarized)

                # 9. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å, vectorizer –∏ mlb
                trainer.save(model, vectorizer, mlb)
                
            # 10. –û—Ü–µ–Ω–∫–∞
            print("–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏:")
            metrics = trainer.evaluate(X_test, y_test, target_names=trainer.mlb.classes_)
        else:
        #####################################################################
        # 5. –ù–ï–ô–†–û–°–ï–¢–ï–í–ê–Ø –ú–û–î–ï–õ–¨ - DeepPavlov/rubert-base-cased
        #####################################################################

            # –ë–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–∫ –ø–æ–ª–Ω–æ—Å—Ç—é –∫–∞–∫ —É —Å—Ç–∞—Ç –º–æ–¥–µ–ª–∏
            nn_mlb = MultiLabelBinarizer()
            y = nn_mlb.fit_transform(filtered_topics_list)
            joblib.dump(nn_mlb, NN_MLB)
            print(f"‚úÖ–ë–∏–Ω–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–ª –º–µ—Ç–∫–∏")

            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
            tokenizer = AutoTokenizer.from_pretrained("DeepPavlov/rubert-base-cased")

            ### # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç—ã —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã:
            ### tokenized_inputs = tokenizer(
            ###     X_train,
            ###     padding="max_length",
            ###     truncation=True,
            ###     max_length=64,
            ###     return_tensors="pt"
            ### )
                       
                        

            def batch_tokenize(texts, tokenizer, batch_size=16, max_length=128):
                """
                –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ—Ä—Ü–∏—è–º–∏.
                –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π: [{'input_ids': ..., 'attention_mask': ...}, ...]
                """
                tokenized_batches = []
                
                for i in range(0, len(texts), batch_size):
                    batch_texts = texts[i : i + batch_size]
                    tokenized = tokenizer(
                        batch_texts,
                        padding="max_length",
                        truncation=True,
                        max_length=max_length,
                        return_tensors="pt"
                    )
                    tokenized_batches.append(tokenized)
                
                return tokenized_batches

            def batchify_labels(labels, batch_size=16):
                """–†–∞–∑–±–∏–≤–∞–µ—Ç –º–∞—Å—Å–∏–≤ –º–µ—Ç–æ–∫ –Ω–∞ –±–∞—Ç—á–∏"""
                label_batches = []
                for i in range(0, len(labels), batch_size):
                    batch_labels = labels[i : i + batch_size]
                    label_batches.append(batch_labels)
                return label_batches

            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
            MAX_LENGTH = 128
            BATCH_SIZE_TOKENIZE = 16
            print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –ø–æ –±–∞—Ç—á–∞–º...")
            tokenized_batches_train = batch_tokenize(X_train, tokenizer, batch_size=BATCH_SIZE_TOKENIZE, max_length=MAX_LENGTH)
            tokenized_batches_val = batch_tokenize(X_val, tokenizer, batch_size=BATCH_SIZE_TOKENIZE, max_length=MAX_LENGTH)
            tokenized_batches_test = batch_tokenize(X_test, tokenizer, batch_size=BATCH_SIZE_TOKENIZE, max_length=MAX_LENGTH)
            print(f"‚úÖ–¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–ª —Ç–µ–∫—Å—Ç")
            
            label_batches_train = batchify_labels(y_train, batch_size=BATCH_SIZE_TOKENIZE)
            label_batches_val = batchify_labels(y_val, batch_size=BATCH_SIZE_TOKENIZE)
            label_batches_test = batchify_labels(y_test, batch_size=BATCH_SIZE_TOKENIZE)
            
            # –°–∫–ª–µ–∏–≤–∞–µ–º –≤—Å–µ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–∞—Ç—á–∏ –≤ –æ–¥–∏–Ω —Ç–µ–Ω–∑–æ—Ä
            input_ids_train = torch.cat([b["input_ids"] for b in tokenized_batches_train])
            input_ids_val = torch.cat([b["input_ids"] for b in tokenized_batches_val])
            input_ids_test = torch.cat([b["input_ids"] for b in tokenized_batches_test])
            attention_mask_train = torch.cat([b["attention_mask"] for b in tokenized_batches_train])
            attention_mask_val = torch.cat([b["attention_mask"] for b in tokenized_batches_val])
            attention_mask_test = torch.cat([b["attention_mask"] for b in tokenized_batches_test])
            labels_tensor_train = torch.tensor(np.array(label_batches_train), dtype=torch.float32)
            labels_tensor_val = torch.tensor(np.array(label_batches_val), dtype=torch.float32)
            labels_tensor_test = torch.tensor(np.array(label_batches_test), dtype=torch.float32)

            # –°–æ–∑–¥–∞—ë–º PyTorch Dataset
            dataset_train = TensorDataset(input_ids_train, attention_mask_train, labels_tensor_train)
            dataset_val = TensorDataset(input_ids_val, attention_mask_val, labels_tensor_val)
            dataset_test = TensorDataset(input_ids_test, attention_mask_test, labels_tensor_test)

            ### # –°–æ–∑–¥–∞—ë–º HuggingFace Dataset
            ### dataset = Dataset.from_dict({
            ###     "input_ids": input_ids,
            ###     "attention_mask": attention_mask,
            ###     "labels": torch.tensor(y)
            ### })
            print(f"‚úÖ–°–æ–±—Ä–∞–ª –¥–∞—Ç–∞—Å–µ—Ç")

            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø–æ–¥ –º—É–ª—å—Ç–∏–ª–µ–π–±–ª
            model = AutoModelForSequenceClassification.from_pretrained(
                "DeepPavlov/rubert-base-cased",
                num_labels=y.shape[1],
                problem_type="multi_label_classification",
                id2label={i: label for i, label in enumerate(nn_mlb.classes_)},
                label2id={label: i for i, label in enumerate(nn_mlb.classes_)}
            )
            print(f"‚úÖ–ó–∞–≥—Ä—É–∑–∏–ª –º—É–ª—å—Ç–∏–ª–µ–π–±–ª –º–æ–¥–µ–ª—å")

            # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –≤ –∫–ª–∞—Å—Å–µ, —Ç–∞–∫ –∫–∞–∫ —É –Ω–∞—Å –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è!
            class MultiLabelTrainer(Trainer):
                #def compute_loss(self, model, inputs, return_outputs=False):
                def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
                    labels = inputs.pop("labels")
                    outputs = model(**inputs)
                    logits = outputs.logits
                    loss = nn.BCEWithLogitsLoss()(logits, labels.float())
                    return (loss, outputs) if return_outputs else loss

            training_args = TrainingArguments(
                output_dir="./results",
                learning_rate=2e-5,
                per_device_train_batch_size=16,
                per_device_eval_batch_size=32,
                num_train_epochs=5,
                weight_decay=0.01,
                eval_strategy="epoch",
                save_strategy="epoch",
                logging_dir="./logs",
                logging_steps=10,
                report_to="tensorboard",
                fp16=True,
                load_best_model_at_end=True,
                metric_for_best_model="f1_micro",
            )
            
            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            def compute_metrics(pred):
                print("–í—ã—á–∏—Å–ª—è—é –º–µ—Ç—Ä–∏–∫–∏...")
                labels = pred.label_ids
                probs = torch.sigmoid(torch.tensor(pred.predictions))
                preds = (probs >= 0.3).int()

                f1_micro = f1_score(labels, preds, average="micro")
                precision_micro = precision_score(labels, preds, average="micro")
                recall_micro = recall_score(labels, preds, average="micro")
                hamming = hamming_loss(labels, preds)

                return {
                    "f1_micro": f1_micro,
                    "precision_micro": precision_micro,
                    "recall_micro": recall_micro,
                    "hamming_loss": hamming
                }

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
            if torch.cuda.is_available():
                device = "cuda"  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º CUDA
                print(f"–ï—â–µ —Ä–∞–∑: GPU –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.get_device_name(0)}. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {device}")
            else:
                device = "cpu"  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º –¶–ü–£
                print(f"–ï—â–µ —Ä–∞–∑: GPU –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {device}")
            
            print(f"‚úÖ–û–±—É—á–µ–Ω–∏–µ: –Ω–∞—á–∞–ª–æ")
            # –û–±—É—á–µ–Ω–∏–µ
            trainer = MultiLabelTrainer(
                model=model,
                args=training_args,
                train_dataset=dataset_train,
                eval_dataset=dataset_val,
                compute_metrics=compute_metrics
            )

            trainer.train()
            print(f"‚úÖ–û–±—É—á–µ–Ω–∏–µ: –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
            model.save_pretrained(NN_MODEL_PATH)
            tokenizer.save_pretrained(NN_MODEL_PATH)
            print(f"‚úÖ–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å")