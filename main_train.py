# main_train.py

import os
import json
import joblib
import shutil
import torch
import torch.nn as nn
import numpy as np

from tqdm import tqdm
from src.config import MAX_WORDS, MAX_LEN
from src.config import PROJECT_ROOT, INPUT_DIR, TEMP_DIR, MIN_DATASET_DATE
from src.config import DATASET_PATH, STAT_MODEL_PATH, STAT_MODEL, VECTORIZER, MLB
from src.config import NN_DATASET_PATH, NN_MODEL_PATH, NN_MODEL, NN_TOKENIZER, NN_MLB
from src.config import NN_TRAIN_BATCHES, NN_TEST_BATCHES, NN_VAL_BATCHES
from src.config import NN_TRAIN_LABELS_BATCHES, NN_TEST_LABELS_BATCHES, NN_VAL_LABELS_BATCHES
from src.config import MODEL_RESULTS, MAX_LENGTH, BATCH_SIZE_TOKENIZE, BATCH_SIZE_TO_MODEL

from src.docx_parser import FieldExtractor
from src.file_preparer import FilePreparer
from src.dataset_builder import DatasetBuilder
from src.dataset_manager import DatasetManager
from src.dataset_batcher import BatchedTextDataset
from src.stat_model_trainer import StatisticalModelTrainer

from datasets import Dataset
from typing import List, Dict
from scipy.sparse import csr_matrix
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from transformers import TrainingArguments, Trainer
from torch.utils.data import TensorDataset, DataLoader

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
choosen_dataset_path = ''
train_model = True              # –§–ª–∞–≥ - –±—ã—Å—Ç—Ä—ã–π –ø—Ä–µ—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –Ω–∞ –¥–∏—Å–∫–µ)
choosen_model = 'nn'            # stat / nn - –≤—ã–±–æ—Ä –æ–±—É—á–∞–µ–º–æ–π/–ø—Ä–∏–º–µ–Ω—è–µ–º–æ–π –º–æ–¥–µ–ª–∏.
need_train = False              # –§–ª–∞–≥ - –Ω—É–∂–Ω–æ –ª–∏ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ –Ω–µ—Ç –Ω–∞ –¥–∏—Å–∫–µ –æ–±—É—á–µ–Ω–Ω–æ–π)
X_train_common = None           # –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
y_train_common = None           # –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç–æ–∫
vectorizer = None               # –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ —Ç–µ–∫—Å—Ç–∞
mlb = None                      # –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ç–æ—Ä–∞ –º–µ—Ç–æ–∫
stat_model = None               # –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç –º–æ–¥–µ–ª–∏
exam_is_done = None             # –≠–∫–∑–∞–º–µ–Ω –≤ –£–ò–ò —Å–¥–∞–Ω? –ü—Ä–æ–ø—É—Å–∫ –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –¥–æ—Ä–∞–±–æ—Ç–æ–∫

if __name__ == "__main__":
    if choosen_model == 'stat':
        choosen_dataset_path = DATASET_PATH
    else:
        choosen_dataset_path = NN_DATASET_PATH
    if 1:
        if train_model:
            manager = DatasetManager(choosen_dataset_path)        
            if (os.path.exists(choosen_dataset_path)):
                updated_dataset = manager.load_dataset()
                print(f"‚úÖ–ó–∞–≥—Ä—É–∑–∏–ª –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ .json")
            else:
                #####################################################################
                # –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–ê
                #####################################################################
                #####################################################################
                # I. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –§–∞–π–ª–æ–≤
                #####################################################################
                # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ (–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ –∞—Ä—Ö–∏–≤–æ–≤ –∏/–∏–ª–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏–∑ .docx)
                
                preparer = FilePreparer()
                docs_path = preparer.prepare_files()   # —Ç—É—Ç –Ω–æ–≤—ã—Ö —É–∂–µ –Ω–µ—Ç

                #####################################################################
                # II. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                #####################################################################
                # 2. –°–±–æ—Ä–∫–∞ –Ω–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
                builder = DatasetBuilder()
            
                print('–ù–∞–π–¥–µ–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –î–°: ', len(docs_path))
                for file_path in tqdm(docs_path, desc="–°–æ–±–∏—Ä–∞–µ–º –î–° –∏–∑ .txt –≤ –µ–¥–∏–Ω—ã–π .json", unit="—Ñ–∞–π–ª"):
                    builder.add_file(file_path)
                new_dataset = builder.build()
                updated_dataset = new_dataset
        if 1:    
            #####################################################################
            # –û–±—Ö–æ–¥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –µ–ª—Å–∏ –µ—Å—Ç—å –¥–∞—Ç—Å–µ—Ç –≤ .json
            #####################################################################
            #### manager = DatasetManager(NN_DATASET_PATH)
            #### if (os.path.exists(NN_DATASET_PATH)):
            ####     updated_dataset = manager.load_dataset()
            #### print(f"‚úÖ–ó–∞–≥—Ä—É–∑–∏–ª –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ .json")

            #####################################################################
            # III. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
            #####################################################################
            if exam_is_done:
                # 3. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–º –ø—Ä–∏ —á–∞—Å—Ç–∏—á–Ω–æ–º –ø–æ–ø–æ–ª–Ω–µ–Ω–∏–∏ (–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ, –±—ç–∫–∞–ø, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ)
                # –ë—É–¥–µ—Ç —Ä–∞–ª–∏–∑–æ–≤–∞–Ω–æ –ø–æ—Å–ª–µ —Å–¥–∞—á–∏ –≠–∫–∑–∞–º–µ–Ω–∞: –≤–∞–∂–Ω–∞—è –≤–µ—â—å, –Ω–æ –Ω–µ –Ω–∞ –Ω–∞—á–∞–ª—å–Ω–æ–º —ç—Ç–∞–ø–µ.
                old_dataset = manager.load_dataset()
                if builder.is_empty():
                    updated_dataset = old_dataset
                    print("–ù–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ—Ç ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –¥–∞—Ç–∞—Å–µ—Ç")
                # –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–æ–≤—ã–µ, —Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ä—ã–µ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö.
                # –ò–∑–º–µ–Ω–µ–Ω–Ω—ã–µ —É–¥–∞–ª–∏—Ç—å –∏–∑ –Ω–æ–≤—ã—Ö –∏ –æ–±–Ω–æ–≤–∏—Ç—å –∏—Ö –∫–æ–ø–∏–∏ –≤ —Å—Ç–∞—Ä–æ–º –î–°
                else:
                    updated_dataset, updated_new_items = manager.update_dataset(old_dataset, new_dataset)
                    print(f"–î–æ–±–∞–≤–ª–µ–Ω–æ –∏–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–æ {len(updated_dataset) - len(old_dataset)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                manager.save_dataset(updated_dataset)
            else:
                manager.save_dataset(updated_dataset)
            print(f"‚úÖ–°–æ—Ö—Ä–∞–Ω–∏–ª –¥–∞—Ç–∞—Å–µ—Ç –≤ .json")
                
            # 4. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (–¥–∞—Ç–∞ / —Ä–µ–¥–∫–∏–µ —Ç–µ–º–∞—Ç–∏–∫–∏) –ø–æ–¥ –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫—É—é –º–æ–¥–µ–ª—å
            filtered_texts_list, filtered_topics_list = manager.prepare_data(updated_dataset, min_date=MIN_DATASET_DATE)
            print(f"‚úÖ–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–ª –î–°")
            print(f"{len(filtered_texts_list)}, {len(filtered_topics_list)}")

            # –ë–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–∫
            nn_mlb = MultiLabelBinarizer()
            binarized_topics_list = nn_mlb.fit_transform(filtered_topics_list)
            joblib.dump(nn_mlb, NN_MLB)
            print(f"‚úÖ–ë–∏–Ω–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–ª –º–µ—Ç–∫–∏")
            
            # 5. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test
            X_train, X_val, X_test, y_train, y_val, y_test = manager.split_dataset(filtered_texts_list, binarized_topics_list)
            print(f"‚úÖ–†–∞–∑–¥–µ–ª–∏–ª –Ω–∞ train/val/test")
            
            print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(updated_dataset)}.")
            print(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –¥–∞—Ç–µ: {len(filtered_texts_list)} (train={len(X_train)}, val={len(X_val)}, test={len(X_test)})")

            #####################################################################
            # –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ê–Ø –ú–û–î–ï–õ–¨ LogisticRegression()
            #####################################################################
            if choosen_model == 'stat':
                #####################################################################
                # IV. –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
                #####################################################################
                # 5. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –¥–∏—Å–∫–∞...
                os.makedirs(STAT_MODEL_PATH, exist_ok=True)
                trainer = StatisticalModelTrainer()
                if (os.path.exists(VECTORIZER) and os.path.exists(MLB) and os.path.exists(STAT_MODEL)):
                    trainer.load()
                    print(f"‚úÖ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –∏ –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —Å –¥–∏—Å–∫–∞: {STAT_MODEL_PATH}")
                # –µ—Å–ª–∏ –Ω–∞ –¥–∏—Å–∫–µ –Ω–µ—Ç –æ–±—É—á–µ–Ω–Ω–æ–π –°–ú, —Ç–æ –æ–±—É—á–∞–µ–º –µ–µ...
                else:
                    vectorizer, mlb, model = trainer.vectorizer, trainer.mlb, trainer.model
                    print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å...")

                    # 6. –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
                    X_train_vectorized, y_train_binarized = trainer.vectorize_dataset(X_train, y_train)

                    # 8. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
                    trainer.train(X_train_vectorized, y_train_binarized)

                    # 9. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å, vectorizer –∏ mlb
                    trainer.save()
                    
                # 10. –û—Ü–µ–Ω–∫–∞
                print("–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏:")
                metrics = trainer.evaluate(X_test, y_test)
            else:
            #####################################################################
            # 5. –ù–ï–ô–†–û–°–ï–¢–ï–í–ê–Ø –ú–û–î–ï–õ–¨ - DeepPavlov/rubert-base-cased
            #####################################################################
                ### # –ë–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–∫ –ø–æ–ª–Ω–æ—Å—Ç—é –∫–∞–∫ —É —Å—Ç–∞—Ç –º–æ–¥–µ–ª–∏
                ### nn_mlb = MultiLabelBinarizer()
                ### y = nn_mlb.fit_transform(filtered_topics_list)
                ### joblib.dump(nn_mlb, NN_MLB)
                ### print(f"‚úÖ–ë–∏–Ω–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–ª –º–µ—Ç–∫–∏")

                # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
                tokenizer = AutoTokenizer.from_pretrained("DeepPavlov/rubert-base-cased")

                # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
                batcher = BatchedTextDataset(BATCH_SIZE_TOKENIZE)

                print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –ø–æ –±–∞—Ç—á–∞–º...")
                if BatchedTextDataset.batches_exist(NN_TRAIN_BATCHES, NN_TRAIN_LABELS_BATCHES):
                    print("–ë–∞—Ç—á–∏ —Ç—Ä–µ–π–Ω–∞ —É–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
                else:
                    print("–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É—é —Ç—Ä–µ–π–Ω...")
                    batcher.build_text_batches(X_train, tokenizer, max_length=MAX_LENGTH, batches_path=NN_TRAIN_BATCHES)

                #batcher.build_text_batches(X_train, tokenizer, max_length=MAX_LENGTH, batches_path=NN_TRAIN_BATCHES)
                print("–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É—é –≤–∞–ª...")
                batcher.build_text_batches(X_val, tokenizer, max_length=MAX_LENGTH, batches_path=NN_VAL_BATCHES)
                print("–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É—é —Ç–µ—Å—Ç...")
                batcher.build_text_batches(X_test, tokenizer, max_length=MAX_LENGTH, batches_path=NN_TEST_BATCHES)
                print(f"‚úÖ–¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–ª —Ç–µ–∫—Å—Ç")
                
                print("üöÄ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∫–∏ –ø–æ –±–∞—Ç—á–∞–º...")
                if BatchedTextDataset.batches_exist(NN_TRAIN_BATCHES, NN_TRAIN_LABELS_BATCHES):
                    print("–ú–µ—Ç–∫–∏ —Ç—Ä–µ–π–Ω–∞ —É–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
                else:
                    print("–°–æ—Ö—Ä–∞–Ω—è—é –º–µ—Ç–∫–∏ —Ç—Ä–µ–π–Ω–∞...")
                    batcher.build_label_batches(y_train, batches_path=NN_TRAIN_LABELS_BATCHES)

                #batcher.build_label_batches(y_train, batches_path=NN_TRAIN_LABELS_BATCHES)
                print("–°–æ—Ö—Ä–∞–Ω—è—é –º–µ—Ç–∫–∏ –≤–∞–ª–∞...")
                batcher.build_label_batches(y_val, batches_path=NN_VAL_LABELS_BATCHES)
                print("–°–æ—Ö—Ä–∞–Ω—è—é –º–µ—Ç–∫–∏ —Ç–µ—Å—Ç–∞...")
                batcher.build_label_batches(y_test, batches_path=NN_TEST_LABELS_BATCHES)

                print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–∏–ª –º–µ—Ç–∫–∏ –ø–æ –±–∞—Ç—á–∞–º")

                # –°–æ–∑–¥–∞—ë–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –±–∞—Ç—á–µ–π
                train_dataset = BatchedTextDataset(BATCH_SIZE_TOKENIZE)
                train_dataset.load_from_disk(batches_dir=NN_TRAIN_BATCHES, labels_dir=NN_TRAIN_LABELS_BATCHES)
                # –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
                #print("–ü—Ä–∏–º–µ—Ä –º–µ—Ç–æ–∫ –∏–∑ train_dataset[0]:", train_dataset[0]['labels'])

                val_dataset = BatchedTextDataset(BATCH_SIZE_TOKENIZE)
                val_dataset.load_from_disk(batches_dir=NN_VAL_BATCHES, labels_dir=NN_VAL_LABELS_BATCHES)
                # –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
                #print("–ü—Ä–∏–º–µ—Ä –º–µ—Ç–æ–∫ –∏–∑ val_dataset[0]:", val_dataset[0]['labels'])

                test_dataset = BatchedTextDataset(BATCH_SIZE_TOKENIZE)
                test_dataset.load_from_disk(batches_dir=NN_TEST_BATCHES, labels_dir=NN_TEST_LABELS_BATCHES)
                # –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
                #print("–ü—Ä–∏–º–µ—Ä –º–µ—Ç–æ–∫ –∏–∑ test_dataset[0]:", test_dataset[0]['labels'])
                
                print(f"‚úÖ–°–æ–±—Ä–∞–ª –¥–∞—Ç–∞—Å–µ—Ç")

                # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø–æ–¥ –º—É–ª—å—Ç–∏–ª–µ–π–±–ª
                model = AutoModelForSequenceClassification.from_pretrained(
                    "DeepPavlov/rubert-base-cased",
                    num_labels=binarized_topics_list.shape[1],
                    #num_labels=y.shape[1],
                    problem_type="multi_label_classification",
                    id2label={i: label for i, label in enumerate(nn_mlb.classes_)},
                    label2id={label: i for i, label in enumerate(nn_mlb.classes_)}
                )
                model.to("cuda" if torch.cuda.is_available() else "cpu")
                print(f"‚úÖ–ó–∞–≥—Ä—É–∑–∏–ª –º—É–ª—å—Ç–∏–ª–µ–π–±–ª –º–æ–¥–µ–ª—å")

                # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –≤ –∫–ª–∞—Å—Å–µ, —Ç–∞–∫ –∫–∞–∫ —É –Ω–∞—Å –º–Ω–æ–≥–æ–º–µ—Ç–æ—á–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è!
                class MultiLabelTrainer(Trainer):
                    #def compute_loss(self, model, inputs, return_outputs=False):
                    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
                        labels = inputs.pop("labels").float()
                        outputs = model(**inputs)
                        logits = outputs.logits
                        loss = nn.BCEWithLogitsLoss()(logits, labels.float())
                        #labels = labels.float() if labels.dtype == torch.long else labels
                        #loss = nn.BCEWithLogitsLoss()(logits, labels)                    
                        return (loss, outputs) if return_outputs else loss

                training_args = TrainingArguments(
                    output_dir=MODEL_RESULTS,
                    learning_rate=0.001,
                    per_device_train_batch_size=BATCH_SIZE_TO_MODEL,
                    per_device_eval_batch_size=BATCH_SIZE_TO_MODEL,
                    num_train_epochs=50,
                    weight_decay=0.01,
                    eval_strategy="epoch",
                    save_strategy="epoch",
                    logging_dir="./logs",
                    logging_steps=1,
                    report_to="tensorboard",
                    fp16=torch.cuda.is_available(),  
                    load_best_model_at_end=True,
                    metric_for_best_model="f1_micro",
                    remove_unused_columns=False, 
                    torch_compile=True, # optimizations
                    optim="adamw_torch_fused", # improved optimizer
                )
                
                # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
                def compute_metrics(pred):
                    print("–í—ã—á–∏—Å–ª—è—é –º–µ—Ç—Ä–∏–∫–∏...")
                    labels = pred.label_ids
                    probs = torch.sigmoid(torch.tensor(pred.predictions))

                    # üîç –ü—Ä–æ–≤–µ—Ä–∫–∞: –∫–∞–∫–∏–µ –º–µ—Ç–∫–∏ –ø—Ä–∏—à–ª–∏?
                    print("–ü—Ä–∏–º–µ—Ä –º–µ—Ç–æ–∫ (–ø–µ—Ä–≤—ã–µ 5):", labels[:5].sum(axis=1))  # –ø–æ—Å–º–æ—Ç—Ä–∏, –µ—Å—Ç—å –ª–∏ –≤–æ–æ–±—â–µ 1 –≤ –º–µ—Ç–∫–∞—Ö
                    print("–ü—Ä–∏–º–µ—Ä probs (–ø–µ—Ä–≤—ã–µ 5):", probs[:5].mean(dim=1))   # —Å—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∞–∫—Ç–∏–≤–∞—Ü–∏–∏

                    preds = (probs >= 0.001).int()

                    f1_micro = f1_score(labels, preds, average="micro")
                    precision_micro = precision_score(labels, preds, average="micro")
                    recall_micro = recall_score(labels, preds, average="micro")
                    hamming = hamming_loss(labels, preds)

                    return {
                        "f1_micro": f1_micro,
                        "precision_micro": precision_micro,
                        "recall_micro": recall_micro,
                        "hamming_loss": hamming
                    }

                ### # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
                ### if torch.cuda.is_available():
                ###     device = "cuda"  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º CUDA
                ###     print(f"–ï—â–µ —Ä–∞–∑: GPU –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.get_device_name(0)}. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {device}")
                ### else:
                ###     device = "cpu"  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º –¶–ü–£
                ###     print(f"–ï—â–µ —Ä–∞–∑: GPU –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {device}")

                print(f"‚úÖ–û–±—É—á–µ–Ω–∏–µ: –Ω–∞—á–∞–ª–æ")
                # –û–±—É—á–µ–Ω–∏–µ
                trainer = MultiLabelTrainer(
                    model=model,
                    args=training_args,
                    train_dataset=train_dataset,
                    eval_dataset=val_dataset,
                    compute_metrics=compute_metrics
                )

                trainer.train()
                if 1==0:
                    # –ó–∞–≥—Ä—É–∑–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ç—á –∏–∑ —Ç—Ä–µ–π–Ω–∞
                    first_batch = train_dataset[len(train_dataset) - 1]
                    print("input_ids shape:", first_batch["input_ids"].shape)
                    print("labels shape:", first_batch["labels"].shape)

                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤—Å–µ —Ç–µ–Ω–∑–æ—Ä—ã –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
                    first_batch = {k: v.to(model.device) for k, v in first_batch.items()}

                    # –£–±–∏—Ä–∞–µ–º unsqueeze, –µ—Å–ª–∏ –Ω–µ –Ω—É–∂–µ–Ω (–µ—Å–ª–∏ –±–∞—Ç—á —É–∂–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞)
                    outputs = model(**first_batch)
                    logits = outputs.logits

                    # –ü–æ—Å—á–∏—Ç–∞–µ–º loss
                    loss = nn.BCEWithLogitsLoss()(logits, first_batch['labels'].float())
                    print("Loss:", loss.item())
                                
                print(f"‚úÖ–û–±—É—á–µ–Ω–∏–µ: –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
                model.save_pretrained(NN_MODEL_PATH)
                tokenizer.save_pretrained(NN_MODEL_PATH)
                print(f"‚úÖ–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å")

            
                # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–µ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
                print("üß™ –û—Ü–µ–Ω–∏–≤–∞—é –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ...")
                print(f"test_dataset={len(test_dataset)}")
                test_results = trainer.evaluate(test_dataset)
                print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–µ:", test_results)
                print(f"‚úÖ–ú–æ–¥–µ–ª—å –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞")
